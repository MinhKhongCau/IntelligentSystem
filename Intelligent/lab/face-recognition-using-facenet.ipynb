{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":992580,"sourceType":"datasetVersion","datasetId":543939},{"sourceId":5143137,"sourceType":"datasetVersion","datasetId":2988105}],"dockerImageVersionId":30407,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introduction**\n---\n\n**About the Dataset**\n---\n---\n\nThis dataset contains over **100,000 facial images** of **1,063 different celebrities** and public figures, including actors, musicians, politicians, athletes, and social media influencers. The images are in **JPEG format** and vary in quality, size, and\n setting. The dataset also includes metadata such as the individual's name, gender, and the number of images available for each person. It is designed for research in\n facial recognition, computer vision, and machine learning algorithms for tasks like face recognition and facial expression analysis.\n\n**Problem Statement**\n---\n\n---\n\nThe facial recognition problem in the context of this dataset is to develop a machine learning model that can accurately identify individuals from their facial images. Given a new image of a person's face, the goal of the model would be to correctly identify the person in the image by matching it with the images of known individuals in the dataset.\n\nThe problem of facial recognition has many potential applications, including security systems, access control, personal identification, and more. However, it is important to ensure that these systems are designed and used in an ethical manner, with appropriate safeguards to protect privacy and prevent misuse.\n\nIn this notebook, we will explore the PINS Face Recognition Dataset and propose a solution using the FaceNet model for facial recognition. We will discuss the preprocessing of the data, training the FaceNet model, fine-tuning the model, face recognition, and evaluation of the model's performance. We will also highlight the benefits of u\nsing a pre-trained FaceNet model for facial recognition tasks.\n","metadata":{}},{"cell_type":"markdown","source":"# **SetUp**\n\n---\n\nBefore proceeding further, let's ensure that we have imported all the necessary modules required for our notebook. This will help us work more efficiently.","metadata":{}},{"cell_type":"code","source":"# Common\nimport os\nimport cv2 as cv\nimport numpy as np\nfrom IPython.display import clear_output as cls\n\n# Data \nfrom tqdm import tqdm\nfrom glob import glob\n\n# Data Visuaalization\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\n# Model\nfrom tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:25:58.335896Z","iopub.execute_input":"2024-02-04T09:25:58.336305Z","iopub.status.idle":"2024-02-04T09:26:10.24308Z","shell.execute_reply.started":"2024-02-04T09:25:58.336267Z","shell.execute_reply":"2024-02-04T09:26:10.241488Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting a random\nnp.random.seed(42)\n\n# Define the image dimensions\nIMG_W, IMG_H, IMG_C = (160, 160, 3)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:26:10.245081Z","iopub.execute_input":"2024-02-04T09:26:10.245921Z","iopub.status.idle":"2024-02-04T09:26:10.250619Z","shell.execute_reply.started":"2024-02-04T09:26:10.245878Z","shell.execute_reply":"2024-02-04T09:26:10.249651Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Loading**\n---\n\nLoading the data into memory is a crucial step in face recognition. We typically work with two types of datasets:\n1. the collection of images for testing our model's inference capabilities, and the database, which is a collection of images used to compute the embeddings of faces for face recognition.\n\n2. The database has a significant impact on the performance of our model in two ways. First, the quality of the images in the database can affect our model's ability to recognize faces. Second, the total number of images per individual in the database also affects model performance.\n\n\nIn this dataset, we have a large number of images, and the images vary significantly in terms of lighting, pose, and expression. To account for this diversity, we need a database that is large enough to include embeddings of various diverse images. Therefore, we will use 10 images per individual to ensure that our database is comprehensive and diverse enough to improve model performance.\n\n\nLet's start by collecting all the images present in the data.","metadata":{}},{"cell_type":"code","source":"# Specify the root directory path\nroot_path = '/kaggle/input/pins-face-recognition/105_classes_pins_dataset/'\n\n# Collect all the person names\ndir_names = os.listdir(root_path)\nperson_names = [name.split(\"_\")[-1].title() for name in dir_names]\nn_individuals = len(person_names)\n\nprint(f\"Total number of individuals: {n_individuals}\\n\")\nprint(f\"Name of the individuals : \\n\\t{person_names}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:26:10.251708Z","iopub.execute_input":"2024-02-04T09:26:10.252029Z","iopub.status.idle":"2024-02-04T09:26:10.337346Z","shell.execute_reply.started":"2024-02-04T09:26:10.251995Z","shell.execute_reply":"2024-02-04T09:26:10.336281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This list of names will be used as **labels** to **train our model** to recognize the specific individuals.","metadata":{}},{"cell_type":"code","source":"# Number of images available per person\nn_images_per_person = [len(os.listdir(root_path + name)) for name in dir_names]\nn_images = sum(n_images_per_person)\n\n# Show\nprint(f\"Total Number of Images : {n_images}.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:26:10.339498Z","iopub.execute_input":"2024-02-04T09:26:10.340113Z","iopub.status.idle":"2024-02-04T09:26:18.697386Z","shell.execute_reply.started":"2024-02-04T09:26:10.34007Z","shell.execute_reply":"2024-02-04T09:26:18.695635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the Distribution of number of images per person.\nfig = px.bar(x=person_names, y=n_images_per_person, color=person_names)\nfig.update_layout({'title':{'text':\"Distribution of number of images per person\"}})\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-04T06:23:41.109814Z","iopub.execute_input":"2024-02-04T06:23:41.110779Z","iopub.status.idle":"2024-02-04T06:23:42.93078Z","shell.execute_reply.started":"2024-02-04T06:23:41.110695Z","shell.execute_reply":"2024-02-04T06:23:42.929449Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select all the file paths\nfilepaths = [path  for name in dir_names for path in glob(root_path + name + '/*')]\nnp.random.shuffle(filepaths)\nprint(f\"Total number of images to be loaded : {len(filepaths)}\")\n\n# Create space for the images\nall_images = np.empty(shape=(len(filepaths), IMG_W, IMG_H, IMG_C), dtype = np.float32)\nall_labels = np.empty(shape=(len(filepaths), 1), dtype = np.int32)\n\n# For each path, load the image and apply some preprocessing.\nfor index, path in tqdm(enumerate(filepaths), desc=\"Loading Data\"):\n    \n    # Extract label\n    label = [name[5:] for name in dir_names if name in path][0]\n    label = person_names.index(label.title())\n    \n    # Load the Image\n    image = plt.imread(path)\n    \n    # Resize the image\n    image = cv.resize(image, dsize = (IMG_W, IMG_H))\n    \n    # Convert image stype\n    image = image.astype(np.float32)/255.0\n    \n    # Store the image and the label\n    all_images[index] = image\n    all_labels[index] = label","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:29:45.890723Z","iopub.execute_input":"2024-02-04T09:29:45.891101Z","iopub.status.idle":"2024-02-04T09:32:18.865414Z","shell.execute_reply.started":"2024-02-04T09:29:45.891069Z","shell.execute_reply":"2024-02-04T09:32:18.863762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Visualization**\n---\n\nNow that we have our data set loaded, we can move on to visualizing it. Visualization is an important step in data analysis, as it helps us gain insight into the data and identify any patterns or anomalies. In the context of face recognition, visualization allows us to assess the quality of the images and how well they represent each individual.","metadata":{}},{"cell_type":"code","source":"def show_data(\n    images: np.ndarray, \n    labels: np.ndarray,\n    GRID: tuple=(15,6),\n    FIGSIZE: tuple=(25,50), \n    recog_fn = None,\n    database = None\n) -> None:\n    \n    \"\"\"\n    Function to plot a grid of images with their corresponding labels.\n\n    Args:\n        images (numpy.ndarray): Array of images to plot.\n        labels (numpy.ndarray): Array of corresponding labels for each image.\n        GRID (tuple, optional): Tuple with the number of rows and columns of the plot grid. Defaults to (15,6).\n        FIGSIZE (tuple, optional): Tuple with the size of the plot figure. Defaults to (30,50).\n        recog_fn (function, optional): Function to perform face recognition. Defaults to None.\n        database (dictionary, optional): Dictionary with the encoding of the images for face recognition. Defaults to None.\n\nReturns:\n        None\n    \"\"\"\n    \n    # Plotting Configuration\n    plt.figure(figsize=FIGSIZE)\n    n_rows, n_cols = GRID\n    n_images = n_rows * n_cols\n    \n    # loop over the images and labels\n    for index in range(n_images):\n        \n        # Select image in the corresponding label randomly\n        image_index = np.random.randint(len(images))\n        image, label = images[image_index], person_names[int(labels[image_index])]\n        \n        # Create a Subplot\n        plt.subplot(n_rows, n_cols, index+1)\n        \n        # Plot Image\n        plt.imshow(image)\n        plt.axis('off')\n        \n        if recog_fn is None:\n            # Plot title\n            plt.title(label)\n        else:\n            recognized = recog_fn(image, database)\n            plt.title(f\"True:{label}\\nPred:{recognized}\")\n    \n    # Show final Plot\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:29:42.063683Z","iopub.execute_input":"2024-02-04T09:29:42.064109Z","iopub.status.idle":"2024-02-04T09:29:42.075181Z","shell.execute_reply.started":"2024-02-04T09:29:42.06407Z","shell.execute_reply":"2024-02-04T09:29:42.073961Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_data(images = all_images, labels = all_labels)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T06:57:28.142071Z","iopub.execute_input":"2024-02-04T06:57:28.142541Z","iopub.status.idle":"2024-02-04T06:57:38.769645Z","shell.execute_reply.started":"2024-02-04T06:57:28.142506Z","shell.execute_reply":"2024-02-04T06:57:38.768269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This **data set** is a **treasure trove of images**, containing a **vast variety of photographs**. The **diversity** of the **images** is **remarkable**, with a **range of lighting conditions, poses, and unique facial features** that **distinguish each individual**. It's fascinating to see how these **well-known figures** are captured in **different moments and situations**. As a **fan of many of these individuals**, it's a **delight to explore** the **collection** and see some of **their candid and posed shots**.","metadata":{}},{"cell_type":"markdown","source":"# **Face Database**\n---\n\nIn order to create an effective face recognition model, it is essential to have a diverse and comprehensive database of images. However, manually selecting images for such a large dataset can be a daunting task. To streamline the process, we have opted to randomly choose 10 images per person from our extensive collection. This approach not only saves time and effort, but also ensures that we have a representative sample of each individual's facial features, expressions, and poses. Once the images are selected, we will use the average encoding produced by them for comparison, which will help us improve the accuracy of our face recognition model. Please note that for computational efficiency, we will only save the encodings of these images, not the images themselves.","metadata":{}},{"cell_type":"code","source":"def load_image(image_path: str, IMG_W: int = IMG_W, IMG_H: int = IMG_H) -> np.ndarray:\n    \"\"\"Load and preprocess image.\n    \n    Args:\n        image_path (str): Path to image file.\n        IMG_W (int, optional): Width of image. Defaults to 160.\n        IMG_H (int, optional): Height of image. Defaults to 160.\n    \n    Returns:\n        np.ndarray: Preprocessed image.\n    \"\"\"\n    \n    # Load the image\n    image = plt.imread(image_path)\n    \n    # Resize the image\n    image = cv.resize(image, dsize=(IMG_W, IMG_H))\n    \n    # Convert image type and normalize pixel values\n    image = image.astype(np.float32) / 255.0\n    \n    return image\n\ndef image_to_embedding(image: np.ndarray, model) -> np.ndarray:\n    \"\"\"Generate face embedding for image.\n    \n    Args:\n        image (np.ndarray): Image to generate encoding for.\n        model : Pretrained face recognition model.\n    \n    Returns:\n        np.ndarray: Face embedding for image.\n    \"\"\"\n    \n    # Obtain image encoding\n    embedding = model.predict(image[np.newaxis,...])\n    \n    # Normalize bedding using L2 norm.\n    embedding /= np.linalg.norm(embedding, ord=2)\n    \n    # Return embedding\n    return embedding\n    \ndef generate_avg_embedding(image_paths: list, model) -> np.ndarray:\n    \"\"\"Generate average face embedding for list of images.\n    \n    Args:\n        image_paths (list): List of paths to image files.\n        model : Pretrained face recognition model.\n    \n    Returns:\n        np.ndarray: Average face embedding for images.\n    \"\"\"\n    \n    # Collect embeddings\n    embeddings = np.empty(shape=(len(image_paths), 128))\n    \n    # Loop over images\n    for index, image_path in enumerate(image_paths):\n        \n        # Load the image\n        image = load_image(image_path)\n        \n        # Generate the embedding\n        embedding = image_to_embedding(image, model)\n        \n        # Store the embedding\n        embeddings[index] = embedding\n        \n    # Compute average embedding\n    avg_embedding = np.mean(embeddings, axis=0)\n    \n    # Clear Output\n    cls()\n    \n    # Return average embedding\n    return avg_embedding","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:26:18.701562Z","iopub.execute_input":"2024-02-04T09:26:18.701937Z","iopub.status.idle":"2024-02-04T09:26:18.714564Z","shell.execute_reply.started":"2024-02-04T09:26:18.701902Z","shell.execute_reply":"2024-02-04T09:26:18.712621Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading the **FaceNet** model:","metadata":{}},{"cell_type":"code","source":"# Load model\nmodel = load_model('/kaggle/input/facenet-keras/facenet_keras.h5')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:26:18.71691Z","iopub.execute_input":"2024-02-04T09:26:18.717376Z","iopub.status.idle":"2024-02-04T09:26:23.869888Z","shell.execute_reply.started":"2024-02-04T09:26:18.717335Z","shell.execute_reply":"2024-02-04T09:26:23.86824Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select all the file paths : 50 images per person.\nfilepaths = [np.random.choice(glob(root_path + name + '/*'), size=10) for name in dir_names]\n\n# Create data base\ndatabase = {name:generate_avg_embedding(paths, model=model) for paths, name in tqdm(zip(filepaths, person_names), desc=\"Generating Embeddings\")}","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:26:25.754668Z","iopub.execute_input":"2024-02-04T09:26:25.755041Z","iopub.status.idle":"2024-02-04T09:28:37.406764Z","shell.execute_reply.started":"2024-02-04T09:26:25.755013Z","shell.execute_reply":"2024-02-04T09:28:37.406093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Face Recognition**\n---","metadata":{}},{"cell_type":"markdown","source":"Facial recognition technology is an area of rapid development that employs machine learning algorithms to authenticate and validate individuals based on their facial characteristics. This technology is widely used in various applications, including unlocking smartphones and surveillance systems.\n\nThere are two primary types of facial recognition problems: face verification and face recognition. Face verification involves confirming a person's identity, such as when passing through customs or using facial recognition to unlock a phone. On the other hand, face recognition is a multi-class problem that aims to identify an individual from a large pool of people.\n\nThe success of facial recognition technology is attributed to the use of deep neural networks, such as the Siamese Network. This type of neural network architecture is commonly used for tasks involving similarity or distance measurement, as it consists of two or more identical subnetworks that share the same weights and architecture.\n\nIn the context of facial recognition, a Siamese Network takes two facial images as input and learns to output a similarity score indicating how similar the two images are in terms of facial features. This network has also been applied to other tasks, such as text similarity and signature verification, making it a valuable tool for various similarity and distance measurement tasks.\n\nThe use of a Siamese network for facial recognition involves generating 128-dimensional embeddings for all images in a database. When a new image is inputted, an embedding is produced and compared with the rest of the embeddings in the database to perform facial recognition with a high degree of accuracy. This comparison is made possible by the rich information contained in the embeddings about facial features and their relationships.","metadata":{}},{"cell_type":"code","source":"def compare_embeddings(embedding_1: np.ndarray, embedding_2: np.ndarray, threshold: float = 0.8) -> int:\n    \"\"\"\n    Compares two embeddings and returns 1 if the distance between them is less than the threshold, else 0.\n\n    Args:\n    - embedding_1: A 128-dimensional embedding vector.\n    - embedding_2: A 128-dimensional embedding vector.\n    - threshold: A float value representing the maximum allowed distance between embeddings for them to be considered a match.\n\n    Returns:\n    - 1 if the distance between the embeddings is less than the threshold, else 0.\n    \"\"\"\n\n    # Calculate the distance between the embeddings\n    embedding_distance = embedding_1 - embedding_2\n\n    # Calculate the L2 norm of the distance vector\n    embedding_distance_norm = np.linalg.norm(embedding_distance)\n\n    # Return 1 if the distance is less than the threshold, else 0\n    return embedding_distance_norm if embedding_distance_norm < threshold else 0","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:33:14.896219Z","iopub.execute_input":"2024-02-04T09:33:14.896623Z","iopub.status.idle":"2024-02-04T09:33:14.905011Z","shell.execute_reply.started":"2024-02-04T09:33:14.896582Z","shell.execute_reply":"2024-02-04T09:33:14.903018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def recognize_face(image: np.ndarray, database: dict, threshold: float = 1.0, model = model) -> str:\n    \"\"\"\n    Given an image, recognize the person in the image using a pre-trained model and a database of known faces.\n    \n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        database (dict): A dictionary containing the embeddings of known faces.\n        threshold (float): The distance threshold below which two embeddings are considered a match.\n        model (keras.Model): A pre-trained Keras model for extracting image embeddings.\n        \n    Returns:\n        str: The name of the recognized person, or \"No Match Found\" if no match is found.\n    \"\"\"\n    \n    # Generate embedding for the new image\n    image_emb = image_to_embedding(image, model)\n    \n    # Clear output\n    cls()\n    \n    # Store distances\n    distances = []\n    names = []\n    \n    # Loop over database\n    for name, embed in database.items():\n        \n        # Compare the embeddings\n        dist = compare_embeddings(embed, image_emb, threshold=threshold)\n        \n        if dist > 0:\n            # Append the score\n            distances.append(dist)\n            names.append(name)\n    \n    # Select the min distance\n    if distances:\n        min_dist = min(distances)\n    \n        return names[distances.index(min_dist)].title().strip()\n    \n    return \"No Match Found\"","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:33:16.538595Z","iopub.execute_input":"2024-02-04T09:33:16.538983Z","iopub.status.idle":"2024-02-04T09:33:16.547641Z","shell.execute_reply.started":"2024-02-04T09:33:16.538949Z","shell.execute_reply":"2024-02-04T09:33:16.546297Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's have a quick look at the functioning of the function.","metadata":{}},{"cell_type":"code","source":"# Randomly select an index\nindex = np.random.randint(len(all_images))\n\n# Obtain an image and its corresponding label\nimage_ = all_images[index]\nlabel_ = person_names[int(all_labels[index])]\n\n# Recognize the face in the image\ntitle = recognize_face(image_, database)\n\n# Plot the image along with its true and predicted labels\nplt.imshow(image_)\nplt.title(f\"True:{label_}\\nPred:{title}\")\nplt.axis('off')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-04T09:33:18.705225Z","iopub.execute_input":"2024-02-04T09:33:18.705616Z","iopub.status.idle":"2024-02-04T09:33:18.987595Z","shell.execute_reply.started":"2024-02-04T09:33:18.705583Z","shell.execute_reply":"2024-02-04T09:33:18.986357Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_data(all_images, all_labels, recog_fn = recognize_face, database = database)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-04T09:33:23.13143Z","iopub.execute_input":"2024-02-04T09:33:23.13289Z","iopub.status.idle":"2024-02-04T09:33:42.932098Z","shell.execute_reply.started":"2024-02-04T09:33:23.1328Z","shell.execute_reply":"2024-02-04T09:33:42.929892Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Analysis**\n---\nThis model is exceptional in its capacity to precisely identify almost all individuals in the pictures. While there are a few mistakes, a broader analysis of the images reveals that the model performs exceptionally well overall. This level of performance is likely due to the mean embeddings that were computed for each person. These encodings encompass the average of any variations within an image, enabling the model to be resilient and accurately recognize individuals.\n\nThe model's effectiveness is mainly attributed to its emphasis on computing the similarities between embeddings and comparing the distances between them, which significantly enhances its speed and accuracy. Overall, this model is a highly efficient tool for facial recognition and showcases the power of machine learning in identifying individuals with remarkable precision.","metadata":{}},{"cell_type":"markdown","source":"# **Accuracy** \nLet's find the accuracy of the model. The **accuracy of the model** is also calculated on a **subset of the dataset**, which is selected randomly. However, when these **subsets are averaged out**, the accuracy remains approximately the same. Therefore, below we are only presenting the accuracy of the chosen subset.","metadata":{}},{"cell_type":"code","source":"# Count the number of images\nn_images = 50\n\n# Initialize the number of correct predictions\nn_correct = 0\n\n# Randomly Select images\nindicies = np.random.permutation(n_images)\ntemp_images = all_images[indicies]\ntemp_labels = all_labels[indicies]\n\n# Iterate over each image and its corresponding label\nfor (image, label) in zip(temp_images, temp_labels):\n    \n    # Extract the true label of the person in the image\n    true_label = person_names[int(label)]\n\n    # Use the recognize_face function to predict the label of the person in the image\n    pred_label = recognize_face(image, database)\n\n    # If the true label and the predicted label match, increment the number of correct predictions\n    if true_label == pred_label:\n        n_correct += 1\n\n# Calculate the accuracy of the model\nacc = (n_correct / n_images) * 100.0\n\n# Print the accuracy of the model\nprint(f\"Model Accuracy: {acc}%!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:34:18.457029Z","iopub.execute_input":"2024-02-04T09:34:18.457388Z","iopub.status.idle":"2024-02-04T09:34:24.298799Z","shell.execute_reply.started":"2024-02-04T09:34:18.457359Z","shell.execute_reply":"2024-02-04T09:34:24.297976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Working With Large Database**\n---","metadata":{}},{"cell_type":"markdown","source":"As we discussed earlier, the **size of the database** plays a **crucial role in** **facial recognition accuracy**. To further **analyze the performance** of our model, we can **increase the size of the database** and **observe its impact on the accuracy**. A **larger database** can help the **model learn** and **recognize faces** more **effectively,** leading to improved accuracy** in facial recognition tasks**.","metadata":{}},{"cell_type":"code","source":"# Select all the file paths : 50 images per person.\nfilepaths = [np.random.choice(glob(root_path + name + '/*'), size=50) for name in dir_names]\n\n# Create data base\nlarge_database = {name:generate_avg_embedding(paths, model=model) for paths, name in tqdm(zip(filepaths, person_names), desc=\"Generating Embeddings\")}","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:34:27.243399Z","iopub.execute_input":"2024-02-04T09:34:27.243864Z","iopub.status.idle":"2024-02-04T09:44:52.881081Z","shell.execute_reply.started":"2024-02-04T09:34:27.24381Z","shell.execute_reply":"2024-02-04T09:44:52.88029Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_data(all_images, all_labels, recog_fn = recognize_face, database = large_database)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-04T09:44:59.558586Z","iopub.execute_input":"2024-02-04T09:44:59.55902Z","iopub.status.idle":"2024-02-04T09:45:19.092804Z","shell.execute_reply.started":"2024-02-04T09:44:59.558979Z","shell.execute_reply":"2024-02-04T09:45:19.091237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Having a **sufficient database** is **crucial for accurate face recognition**. In order to improve our model's performance, we **increased the size of our database by computing 50 embeddings per person and averaging them to obtain a single embedding per person**. By doing so, we were able to **significantly improve the accuracy of our model**.","metadata":{}},{"cell_type":"code","source":"# Count the number of images\nn_images = 100\n\n# Initialize the number of correct predictions\nn_correct = 0\n\n# Randomly Select images\nindicies = np.random.permutation(n_images)\ntemp_images = all_images[indicies]\ntemp_labels = all_labels[indicies]\n\n# Iterate over each image and its corresponding label\nfor (image, label) in zip(temp_images, temp_labels):\n    \n    # Extract the true label of the person in the image\n    true_label = person_names[int(label)]\n\n    # Use the recognize_face function to predict the label of the person in the image\n    pred_label = recognize_face(image, large_database)\n\n    # If the true label and the predicted label match, increment the number of correct predictions\n    if true_label == pred_label:\n        n_correct += 1\n\n# Calculate the accuracy of the model\nacc = (n_correct / n_images) * 100.0\n\n# Print the accuracy of the model\nprint(f\"Model Accuracy: {acc}%!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:45:55.951686Z","iopub.execute_input":"2024-02-04T09:45:55.952535Z","iopub.status.idle":"2024-02-04T09:46:07.639806Z","shell.execute_reply.started":"2024-02-04T09:45:55.952488Z","shell.execute_reply":"2024-02-04T09:46:07.638951Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To further improve the **model's performance**, we increased the **size of the database to 50 embeddings per person and then averaged them to get one embedding per person**. This led to a **noticeable increase in accuracy,** with the previous **accuracy of 90% being improved to 92% by using the larger database**. This demonstrates the importance of having a **large and diverse database** for **face recognition models to achieve higher accuracy**.","metadata":{}},{"cell_type":"code","source":"# Select all the file paths : 50 images per person.\nfilepaths = [np.random.choice(glob(root_path + name + '/*'), size=25) for name in dir_names]\n\n# Create data base\nmed_database = {name:generate_avg_embedding(paths, model=model) for paths, name in tqdm(zip(filepaths, person_names), desc=\"Generating Embeddings\")}","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:46:24.176714Z","iopub.execute_input":"2024-02-04T09:46:24.178119Z","iopub.status.idle":"2024-02-04T09:51:34.919234Z","shell.execute_reply.started":"2024-02-04T09:46:24.178058Z","shell.execute_reply":"2024-02-04T09:51:34.918362Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show_data(all_images, all_labels, recog_fn = recognize_face, database = med_database)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T06:39:13.66508Z","iopub.status.idle":"2024-02-04T06:39:13.66618Z","shell.execute_reply.started":"2024-02-04T06:39:13.665924Z","shell.execute_reply":"2024-02-04T06:39:13.665949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the number of images\nn_images = 100\n\n# Initialize the number of correct predictions\nn_correct = 0\n\n# Randomly Select images\nindicies = np.random.permutation(n_images)\ntemp_images = all_images[indicies]\ntemp_labels = all_labels[indicies]\n\n# Iterate over each image and its corresponding label\nfor (image, label) in zip(temp_images, temp_labels):\n    \n    # Extract the true label of the person in the image\n    true_label = person_names[int(label)]\n\n    # Use the recognize_face function to predict the label of the person in the image\n    pred_label = recognize_face(image, med_database)\n\n    # If the true label and the predicted label match, increment the number of correct predictions\n    if true_label == pred_label:\n        n_correct += 1\n\n# Calculate the accuracy of the model\nacc = (n_correct / n_images) * 100.0\n\n# Print the accuracy of the model\nprint(f\"Model Accuracy: {acc}%!!!\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T09:51:40.835784Z","iopub.execute_input":"2024-02-04T09:51:40.836265Z","iopub.status.idle":"2024-02-04T09:51:52.387253Z","shell.execute_reply.started":"2024-02-04T09:51:40.836223Z","shell.execute_reply":"2024-02-04T09:51:52.386195Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"While the companion shows the benefits of using a **larger dataset**, I still prefer to work with the **medium-sized dataset** for this project. In my opinion, **large datasets** can introduce **some noise** into the **embeddings**, which can **negatively impact** the **accuracy of the model**. Therefore, it is **crucial to ensure** that the **embeddings** are as **accurate as possible. For this reason**, I found that having an **average of 20 embeddings per person is sufficient for achieving high accuracy.**\n\n---\nNote : To obtain a **significant improvement** in the performance of the **face recognition model**, it is **crucial to retrain** it using the **new data set.** In our case, we only used a **pre-trained network** and achieved an **impressive 92% accuracy** on the **face recognition method**. However, if we want to take our **model's accuracy to the next level, we must train it on more extensive and diverse data sets**.\n\n**Retraining the model** on **new data** will help it **learn and recognize different facial features** that were **not present in the original data set**. Moreover, it will help to **eliminate any bias** that **may have existed** in the **previous data set. This bias may** have resulted from the **lack of diversity in the original data set**, leading to **inaccurate or incomplete facial recognition**.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}